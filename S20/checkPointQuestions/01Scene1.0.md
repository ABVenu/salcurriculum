**Scene 1.0**


### **1. What does the “temperature” parameter control in a language model API like OpenAI or Gemini?**

A) The number of API requests allowed per minute
B) The format in which the tokens are returned
C) The randomness or creativity of the model’s responses
D) The hardware temperature of the server running the model

**Answer:** C) The randomness or creativity of the model’s responses

---

### **2. What is the main purpose of using a tool like Ollama in the LLM ecosystem?**

A) To build web interfaces for Gemini and GPT models
B) To fine-tune large models using only browser memory
C) To run large language models like LLaMA or Mistral locally on your machine
D) To convert prompts into SQL queries automatically

**Answer:** C) To run large language models like LLaMA or Mistral locally on your machine

---

### **3. How are prompts typically structured when using the OpenAI or Gemini API?**

A) As XML documents containing prompt and model configuration
B) As a series of bash commands executed on the cloud
C) As JSON payloads with fields like `model`, `messages`, and `temperature`
D) As environment variables defined in a `.env` file

**Answer:** C) As JSON payloads with fields like `model`, `messages`, and `temperature`

---
